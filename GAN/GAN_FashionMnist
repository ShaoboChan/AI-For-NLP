from __future__ import division
from torchvision import models
from torchvision import transforms
from PIL import Image
import argparse
import torch
import torch.nn as nn
import torchvision
import numpy as np
import matplotlib.pyplot as plt
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
batch_size=32
transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.5, 0.5, 0.5),
                        std=(0.5, 0.5, 0.5))
])

mnist_data=torchvision.datasets.FashionMNIST('./FMnist',train=True,download=T
                                             ,transform=transform)
dataloader=torch.utils.data.Dataloader(dataset=mnist_data,batch_size=batch_size,shuffle=True)
image_size=784
hidden_size=256

D=nn.Sequential(
    nn.Linear(image_size,hidden_size),
    nn.LeakyReLU(0.2),
    nn.Linear(hidden_size,hidden_size),
    nn.LeakyReLU(0.2),
    nn.Linear(hidden_size,1),
    nn.sigmoid()
)
latent_size=64
G=nn.Sequential(
    nn.Linear(latent_size,hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size,hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size,image_size),
    nn.Tanh()
)
D=D.to(device)
G=G.to(device)


loss_fn=nn.BCELoss()
d_optimizer=torch.optim.Adam(D.parameters(),lr=0.001)
g_optimizer=torch.optim.Adam(D.parameters(),lr=0.001)

def reset_grad():
    d_optimizer.zero_grad()
    g_optimizer.zero_grad()
steps=len(dataloader)
epochs=200

for epoch in range(epochs):
    for i,(images,_) in enumerate(dataloader):
        batch_size=image_size(0)
        images=images.reshape(batch_size,image_size).to(device)#image_size=28*28;batch_size=32

        real_label=torch.ones(batch_size,1).to(device)
        fake_label=torch.zeros(batch_size,1).to(device)


        output=D(images)
        d_loss_real=loss_fn(output,real_label)
        real_score=output


        #fake_image
        z=torch.randn(batch_size,latent_size).to(device)
        fake_image=G(z)
        output=D(fake_image.detach())#隔断
        d_loss_fake=loss_fn(output,fake_label)
        fake_score=output
        ##
        d_loss=d_loss_real+d_loss_fake
        reset_grad()
        d_loss.backward()
        d_optimizer.step()

        ###generator
        z=torch.rand(batch_size,latent_size).to(device)#也可以用钱买生成的z
        fake_image=G(z)
        output=D(z)
        g_loss=loss_fn(output,real_label)


        # d_optimizer.zero_grad()
        # g_optimizer.zero_grad()
        reset_grad()

        g_loss.backward()
        g_optimizer.step()
        if i % 1000 == 0:
            print("Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}"
                  .format(epoch, epochs, i, steps, d_loss.item(), g_loss.item(), real_score.mean().item(),
                          fake_score.mean().item()))


z=torch.randn(1,latent_size).to(device)
fake_image=G(z).view(28,28).data.cpu().numpy()
plt.imshow(fake_image)















