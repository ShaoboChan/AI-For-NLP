from __future__ import division
from torchvision import models
from torchvision import transforms
from PIL import Image
import argparse
import torch
import torch.nn as nn
import torchvision
import numpy as np
import matplotlib.pyplot as plt
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
def load_image(image_path,transform=None,max_size=None,shape=None):
    image=Image.open(image_path)
    if max_size:
        scale=max_size/max(image.size)
        size=np.array(image.size)*scale
        image=image.resize(size.astype(int),Image.ANTIALIAS)
    if shape:
        image=image.resize(shape,Image.LANCZOS)
    if transforms:
        image=transform(image).unsqueeze(0)
    return image.to(device)

#
#
def imshow(tensor,title=None):
    image=tensor.cpu().clone()
    image=image.squeeze(0)
    image=unloader(image)
    plt.imshow(image)
    if title is not None:
        plt.title(title)
    plt.pause(10)
class VGGnet(nn.Module):
    def __init__(self):
        super(VGGnet,self).__init__()
        self.select=['0','5','10','19','28']
        self.vgg=models.vgg19(pretrained=True).features
    def forward(self, x):
        features=[]
        for name,layer in self.vgg._modules.items():
            x=layer(x)
            if name in self.select:
                features.append(x)
        return features
        #5 layers in features


if __name__ =='__main__':
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    #content.shape==style.shape
    content = load_image('./yuao.jpg', transform, max_size=250)
    style = load_image('./midEu.jpg', transform, max_size=250, shape=[content.size(2), content.size(3)])
    unloader = transforms.ToPILImage()
    # plt.ion()
    # plt.figure()
    # imshow(content, title='Image')
    #初始化target为接近content的图
    target = content.clone().requires_grad_(True)
    optimizer = torch.optim.Adam([target], lr=0.002, betas=[0.99, 0.999])
    vgg = VGGnet().to(device).eval()
    target_features=vgg(target)
    epochs=1000
    style_weight=10
    for epoch in range(epochs):
        target_features=vgg(target)
        content_features=vgg(content)
        style_features=vgg(style)

        style_loss=0.
        content_loss=0.

        for f1,f2,f3 in zip(target_features,content_features,style_features):
            content_loss+=torch.mean((f1-f2)**2)
            _,c,h,w=f1.size()
            #压扁
            f1=f1.view(c,h*w)
            f3=f3.view(c,h*w)

            f1=torch.mm(f1,f1.t())#c*c
            f3=torch.mm(f3,f3.t())

            style_loss+=torch.mean((f1-f3)**2)/(c*h*w)

        loss=content_loss+style_weight*style_loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if epoch %2==0:
            print("Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}"
             .format(epoch, epochs, content_loss.item(), style_loss.item()))

    denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))
    img = target.clone().squeeze()
    img = denorm(img).clamp_(0, 1)
    plt.figure()
    imshow(img, title='Target Image')



















