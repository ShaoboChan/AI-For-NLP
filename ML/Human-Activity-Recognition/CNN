from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision import datasets, transforms
from torch.utils.data.dataset import Dataset
import runpy
import pandas as pd
import numpy as np
import os


# from data import FeatureDataset
class FeatureDataset(Dataset):
    def __init__(self, dir, transform=None):
        self.df = pd.read_csv(dir, header=None, sep=',')
        self.to_tensor = torch.FloatTensor()
        var_list = []

        for i in range(2, 7353):
            var_list.append(list(map(float, self.df.iloc[i, 1:562])))

        self.features = var_list

        # self.features=df.iloc[2:7353,0:562]
        # self.features=np.array(self.features).astype(np.float32)
        labels = list(map(int, self.df.iloc[2:7353, 564]))
        self.labels = np.array(labels)
        # self.root_dir=root_dir
        # self.labels=df.iloc[2:7353,564]

    def __len__(self):
        return len(self.df) - 4

    def __getitem__(self, idx):
        # if torch.is_tensor(idx):
        #     idx = idx.tolist()
        feature = self.features[idx]
        feature = np.array([feature])
        feature = feature.astype('float')
        # feature=np.tile(feature, (4, 1))
        # feature=feature.reshape(20,28)
        label = self.labels[idx]

        # feature=list(map(float,feature))

        # self.feature=feature.astype('float')

        # feature=self.to_tensor(feature)
        # label=self.labels[idx]
        sample = {'feature': feature, 'label': label}
        return sample










class Vali_FeatureDataset(Dataset):
    def __init__(self, vali_dir, transform=None):
        self.vali_df = pd.read_csv(vali_dir, header=None, sep=',')
        self.to_tensor = torch.FloatTensor()
        vali_var_list = []
        for i in range(2, 2949):
            # Name: 562, Length: 2948, dtype: object
            vali_var_list.append(list(map(float, self.vali_df.iloc[i, 1:562])))

        self.vali_features = vali_var_list
        vali_labels = list(map(int, self.vali_df.iloc[2:2949, 564]))
        self.vali_labels = np.array(vali_labels)

    def __len__(self):
        return len(self.vali_df) - 4

    def __getitem__(self, idx):
        # if torch.is_tensor(idx):
        #     idx = idx.tolist()
        vali_feature = self.vali_features[idx]
        vali_feature = np.array([vali_feature])
        vali_feature = vali_feature.astype('float')
        # vali_feature=np.tile(vali_feature, (4, 1))
        # feature=feature.reshape(20,28)
        vali_label = self.vali_labels[idx]

        # feature=list(map(float,feature))

        # self.feature=feature.astype('float')

        # feature=self.to_tensor(feature)
        # label=self.labels[idx]
        vali_sample = {'feature': vali_feature, 'label': vali_label}
        return vali_sample


# class Net(nn.Module):
#     def _init_(self,input_size,hidden_size):
#         super(Net,self)._init_()
#         self.fc1=nn.Linear(input_size,hidden_size)
#         self.fc2=nn.Linear(hidden_size,hidden_size)
#         self.fc3=nn.Linear(hidden_size,6)
#     def forward(self,x):
#         x=self.fc1(x)
#         x=self.fc2(x)
#         x = self.fc3(x)
#         return x
#
#

# class HARmodel(nn.Module):
#     """Model for human-activity-recognition."""
#     def __init__(self, input_size, num_classes):
#         super().__init__()
#
#         # Extract features, 1D conv layers
#         self.features = nn.Sequential(
#             nn.Conv1d(input_size,8, 16),#
#             nn.ReLU(),
#             nn.Dropout(),
#             nn.Conv1d(8, 8, 16),
#             nn.ReLU(),
#             nn.Dropout(),
#             nn.Conv1d(8, 8, 16),
#             nn.ReLU(),
#             )
#         # Classify output, fully connected layers
#         self.classifier = nn.Sequential(
#         	nn.Dropout(),
#         	nn.Linear(33024, 128),
#         	nn.ReLU(),
#             nn.Linear(128 , 128),
#             nn.ReLU(),
#         	nn.Dropout(),
#             # nn.Linear(2048 , 128),
#             # nn.ReLU(),
#         	# nn.Dropout(),
#         	nn.Linear(128, num_classes),
#         	)
#
#     def forward(self, x):
#     	x = self.features(x)
#     	x = x.view(x.size(0), 33024)
#     	out = self.classifier(x)
#     	return out

class HARmodel(nn.Module):
    def __init__(self):
        super(HARmodel, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv1d(1, 100, 2),
            nn.BatchNorm1d(100),
            nn.ReLU(),
            # nn.Dropout(),
            nn.MaxPool1d(8))
        self.layer2 = nn.Sequential(
            nn.Conv1d(100, 50, 2),
            nn.BatchNorm1d(50),
            nn.ReLU(),
            # nn.Dropout(),
            nn.MaxPool1d(8))
        self.fc = nn.Linear(400, 6)

    def forward(self, x):
        # input.shape:(16,1,425)
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.view(out.size(0), -1)  # torch.Size([16, 400])
        # self.len_Linear=len(out)
        out = self.fc(out)
        return out


def Train(train_loader, valid_loader, model, criterion, optimizer, device, save_model, epochs, save_dir):
    model.train()
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)

    train_losses = []
    print('-------------------start training--------------------------------------------------')
    for epoch in range(epochs):
        model.train()
        scheduler.step()
        # sum_loss=0.
        for i, sample in enumerate(train_loader):
            x = sample['feature']

            output = model(x.float())
            # print(output.size())
            targets = sample['label']
            targets = targets.long()
            # print(targets.size())
            loss = criterion(output, targets)
            # sum_loss+=loss.item()
            optimizer.zero_grad()
            loss.sum().backward()
            # loss.backward()
            # loss.mean().backward()
            optimizer.step()

            if i % 20 == 0:
                print('=====================================训练========================================')
                print("epoch:{}/{}|iter:{}|loss:{}".format(epoch, epochs, i, loss.item()))
        #######################vali################################################

        valid_mean_pts_loss = 0.0
        acc_total = 0.
        valid_loss = 0.
        model.eval()
        with torch.no_grad():
            valid_batch_cnt = 0.

            for valid_idx, valid_sample in enumerate(valid_loader):
                valid_batch_cnt += 1
                feature = valid_sample['feature']
                target_pts = valid_sample['label']
                target_pts = target_pts.long()
                output_pts = model(feature.float())
                rightOrnot=torch.argmax(output_pts,dim=1)

                # print('结果',rightOrnot,'标签',target_pts)
                acc = 0.
                for i in range(len(rightOrnot)):
                    res= rightOrnot[i]==target_pts[i]
                    acc+=res
                # res=rightOrnot == target_pts
                    if i%30==0 and valid_idx%60==0:
                        print('结果：',rightOrnot,rightOrnot.size(),'目标：',target_pts,target_pts.size())
                acc_total+=acc.item()
                # acc_total/=len(rightOrnot)
                valid_loss = criterion(output_pts, target_pts)
                valid_mean_pts_loss += valid_loss.item()
            valid_mean_pts_loss /= valid_batch_cnt * 1.0
            acc_total/=valid_batch_cnt*16
            print('=================================测试========================================================')
            print('valid:pts_loss:{:.6f}|acc:{}'.format(valid_mean_pts_loss,acc_total))
        if save_model:
            saved_model_name = os.path.join(save_dir, 'classi_epoch' + '_' + str(epoch) + '.pt')
            torch.save(model.state_dict(), saved_model_name)
    return loss, 0.5


if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    criterion = nn.CrossEntropyLoss()
    model = HARmodel()
    model = model.float()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999), eps=1e-08, weight_decay=0)
    train_dataset = FeatureDataset('./train_addLabel_int.csv')
    vali_dataset = Vali_FeatureDataset('./test_addLabel_int.csv')
    train_loader = torch.utils.data.DataLoader(train_dataset,
                                               batch_size=16,
                                               num_workers=1,
                                               pin_memory=True,
                                               shuffle=True)
    vali_loader = torch.utils.data.DataLoader(vali_dataset,
                                              batch_size=16,
                                              num_workers=1,
                                              pin_memory=True,
                                              shuffle=True)
    epochs = 1000

    Train(train_loader, vali_loader, model, criterion, optimizer, device, True, epochs, './model3/')

